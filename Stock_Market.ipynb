{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Market.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4uMKBqcAHtKUNwgtocsoF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjun-1510/prdiction-Svm/blob/main/Stock_Market.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpNtAktbnxSH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Author : ARJUN G\n",
        "Task 4 : Stock Market Prediction using Numerical and Textual Analysis\n",
        "GRIP @ The Sparks Foundation"
      ],
      "metadata": {
        "id": "YuvhW52wn4sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technical Stack : Sikit Learn, Numpy Array, Pandas, Matplotlib, Keras, NLTK, Textblob, Xgboost"
      ],
      "metadata": {
        "id": "vWK9iGIooCR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required libraries\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import metrics \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "import xgboost "
      ],
      "metadata": {
        "id": "qsYlGe7OoG82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 : Importing the Numerical dataset and performing Exploratory Analysis"
      ],
      "metadata": {
        "id": "FWcla_qFoKZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe for exploratory analysis\n",
        "df=pd.read_csv('csv\\^BSESN.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5m24arc-oLWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract date frame and plot closing stock price w.r.t time\n",
        "df['Date'] = pd.to_datetime(df.Date,format='%Y-%m-%d')\n",
        "df.index = df['Date']\n",
        "df.dropna(inplace=True)\n",
        "#plot\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(df['Close'], label='Close Price history')"
      ],
      "metadata": {
        "id": "M4GX-ZYXoTSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 : Creating a dataframe for storing the Closing stock data per day"
      ],
      "metadata": {
        "id": "txuw9jetoXat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(df2, look_back=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(df2)-look_back-1):\n",
        "\t\ta = df2[i:(i+look_back), 0]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(df2[i + look_back, 0])\n",
        "\treturn np.array(dataX), np.array(dataY)"
      ],
      "metadata": {
        "id": "GTNx2kVzoYLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 : Data Normalization and Division into Training and Test sets\n"
      ],
      "metadata": {
        "id": "Xm05lMXmod9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "df2 = scaler.fit_transform(df2)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(df2) * 0.67)\n",
        "test_size = len(df2) - train_size\n",
        "train, test = df2[0:train_size,:], df2[train_size:len(df2),:]\n"
      ],
      "metadata": {
        "id": "m2Xw68X3oleN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape into X=t and Y=t+1\n",
        "look_back = 3\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n"
      ],
      "metadata": {
        "id": "gseVQ0Puoqj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 : Creating a LSTM for Numerical Analysis\n"
      ],
      "metadata": {
        "id": "qwNcm4UlovxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(7, input_shape=(look_back, 1)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n"
      ],
      "metadata": {
        "id": "NdAAt6sso2B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 : Making Predictions"
      ],
      "metadata": {
        "id": "dAIifz_Jo96P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(df2)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(df2)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(df2)-1, :] = testPredict\n",
        "# plot baseline and predictions\n",
        "plt.plot(scaler.inverse_transform(df2))\n",
        "plt.plot(trainPredictPlot,color='red')\n",
        "plt.plot(testPredictPlot,color='green')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wdVo7NnYpBcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print(\"Root mean square error = \",trainScore,\" RMSE\")\n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print(\"Root mean square error = \",testScore,\" RMSE\")"
      ],
      "metadata": {
        "id": "1pEwCOt9pFO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6 : Creating a Hybrid model for Numerical and textual Analysis"
      ],
      "metadata": {
        "id": "vwFjGRippKkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "news.drop(0, inplace=True)\n",
        "news.drop('Category', axis = 1, inplace=True)\n",
        "news.info()"
      ],
      "metadata": {
        "id": "3QPZKjFWpVjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news.drop(0, inplace=True)\n",
        "news.drop('Category', axis = 1, inplace=True)\n",
        "news.info()"
      ],
      "metadata": {
        "id": "yzyeZMFzpVL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7 : Text preprocessing"
      ],
      "metadata": {
        "id": "D9QUJ-yIpcTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news.drop(0, inplace=True)\n",
        "news.drop('Category', axis = 1, inplace=True)\n",
        "news.info()"
      ],
      "metadata": {
        "id": "pewiryxQpfdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8 : Adding subjectivity and polarity Scores"
      ],
      "metadata": {
        "id": "4KRQaGp5pkJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions to get the subjectivity and polarity\n",
        "def getSubjectivity(text):\n",
        "  return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "def getPolarity(text):\n",
        "  return  TextBlob(text).sentiment.polarity"
      ],
      "metadata": {
        "id": "a13qFUBYpnJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding subjectivity and polarity columns\n",
        "news['Subjectivity'] = news['News'].apply(getSubjectivity)\n",
        "news['Polarity'] = news['News'].apply(getPolarity)\n",
        "news\n"
      ],
      "metadata": {
        "id": "_RK7klZ2pqO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9 : Visualizing the polarity and Subjectivity scores"
      ],
      "metadata": {
        "id": "RaAeAl-jpuyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,6))\n",
        "news['Polarity'].hist(color = 'red')"
      ],
      "metadata": {
        "id": "b4eSaebzpxwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,6))\n",
        "news['Subjectivity'].hist(color = 'green')"
      ],
      "metadata": {
        "id": "RpZ-vNXKp0e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10 : Performing Sentiment Analysis over the news Headlines"
      ],
      "metadata": {
        "id": "IJp025J3p3CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding sentiment score to news\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "news['Compound'] = [sia.polarity_scores(v)['compound'] for v in news['News']]\n",
        "news['Negative'] = [sia.polarity_scores(v)['neg'] for v in news['News']]\n",
        "news['Neutral'] = [sia.polarity_scores(v)['neu'] for v in news['News']]\n",
        "news['Positive'] = [sia.polarity_scores(v)['pos'] for v in news['News']]\n",
        "news"
      ],
      "metadata": {
        "id": "0EhHF_XQp6c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "I was able to create a hybrid model for stock price/performance prediction using numerical analysis of historical stock prices, and sentimental analysis of news headlines.\n",
        "Thank You"
      ],
      "metadata": {
        "id": "l4jrzzH5qCyc"
      }
    }
  ]
}